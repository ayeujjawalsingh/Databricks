# ğŸ“˜ Delta Change Feed (CDC) in Databricks

Delta Change Feed (DCF) allows you to **capture row-level changes** (INSERT, UPDATE, DELETE) in Delta tables. It helps in building **incremental pipelines** where you read only the changed data, instead of scanning the full table.

---

## ğŸ“Œ Why Use Delta Change Feed?

| Benefit                          | Description                                              |
|----------------------------------|----------------------------------------------------------|
| âœ… Incremental Reads             | Read only rows that changed (not the entire dataset)     |
| âœ… Track Change Type             | Know if the row was inserted, updated, or deleted        |
| âœ… Efficient for ETL             | Reduces cost and processing time                         |
| âœ… Supports Streaming            | Can be used with structured streaming for real-time use  |
| âœ… Works with Delta Tables       | Built-in support in Delta Lake with transaction logs     |

---

## ğŸ§  How Delta Change Feed Works

Delta Lake stores all changes to a table in **delta logs** (versioned commits). When DCF is enabled:

- Every change (insert/update/delete) is recorded
- Metadata like `_change_type`, `_commit_version`, `_commit_timestamp` is tracked
- You can query this changed data using Spark (batch or streaming)

---

## ğŸ› ï¸ Step-by-Step Implementation

### âœ… Step 1: Enable Change Data Feed

```sql
ALTER TABLE <your_table_name>
SET TBLPROPERTIES (
  delta.enableChangeDataFeed = true
);
```

> ğŸ”¸ Must be done **before** you want to track changes.

---

### âœ… Step 2: Perform Some Changes

```sql
-- Insert
INSERT INTO <your_table_name> VALUES (101, 'Amit', 'Pune');

-- Update
UPDATE <your_table_name> SET city = 'Mumbai' WHERE customer_id = 101;

-- Delete
DELETE FROM <your_table_name> WHERE customer_id = 101;
```

These changes will be captured in Delta logs.

---

### âœ… Step 3: Read the Changed Data

#### ğŸ§¾ Batch Read Example

```python
df_changes = spark.read.format("delta") \
    .option("readChangeData", "true") \
    .option("startingVersion", 10) \
    .table("your_table_name")
```

#### ğŸ§¾ Sample Output

| customer\_id | name | city   | \_change\_type |
| ------------ | ---- | ------ | -------------- |
| 101          | Amit | Pune   | insert         |
| 101          | Amit | Mumbai | update         |
| 101          | null | null   | delete         |

---

### âœ… Step 4: Apply Changes Using `MERGE INTO`

```python
df_changes.createOrReplaceTempView("delta_cdc")

spark.sql("""
MERGE INTO target_table AS tgt
USING delta_cdc AS src
ON tgt.customer_id = src.customer_id

WHEN MATCHED AND src._change_type = 'update' THEN UPDATE SET *
WHEN MATCHED AND src._change_type = 'delete' THEN DELETE
WHEN NOT MATCHED AND src._change_type = 'insert' THEN INSERT *
""")
```

This keeps your target table up-to-date with only changed data.

---

### âœ… Step 5: (Optional) Use Delta Change Feed in Streaming

```python
stream_df = spark.readStream.format("delta") \
    .option("readChangeData", "true") \
    .option("startingVersion", 45) \
    .table("your_table_name")
```

This lets you process the changes in real time, ideal for streaming pipelines.

---

## ğŸ“Š Metadata Columns Added by DCF

| Column Name         | Description                                  |
| ------------------- | -------------------------------------------- |
| `_change_type`      | Type of change: `insert`, `update`, `delete` |
| `_commit_version`   | Delta log version where change occurred      |
| `_commit_timestamp` | Timestamp when the change was committed      |

---

## ğŸ’¡ Use Cases of Delta Change Feed

| Use Case                            | Purpose                                                |
| ----------------------------------- | ------------------------------------------------------ |
| ğŸ”„ Incremental ETL Pipelines        | Load only changed data                                 |
| ğŸ“Š Real-Time Dashboards             | Always show fresh, accurate data                       |
| ğŸ” Data Replication / Sync          | Copy changes to other systems like Redshift, Snowflake |
| ğŸ“‚ Audit & Version Tracking         | Know what changed, when, and why                       |
| ğŸš€ Bronze â†’ Silver â†’ Gold Pipelines | Efficiently propagate changes across layers            |

---

## âš ï¸ Things to Keep in Mind

* Change feed is only available **after you enable it**.
* You must provide a **starting version** when querying changes.
* Deletes return rows with `null` values and `_change_type = 'delete'`.
* Itâ€™s only supported on **Delta tables**, not standard Parquet or CSV.

---

## ğŸ§  Summary

| Feature              | Description                                 |
| -------------------- | ------------------------------------------- |
| Purpose              | Track and process changes in Delta tables   |
| Supported Operations | `INSERT`, `UPDATE`, `DELETE`                |
| Usage Type           | Batch and Streaming                         |
| Trigger              | Enable with table property                  |
| Integration          | Works with `MERGE INTO` and Spark Streaming |
