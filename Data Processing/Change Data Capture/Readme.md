# üìò Complete Guide to Change Data Capture (CDC) using Delta Lake CDF in Databricks

This document provides a complete end-to-end understanding of **Change Data Capture (CDC)** in Databricks using **Delta Lake‚Äôs Change Data Feed (CDF)**.

You‚Äôll learn:

- ‚úÖ What is CDC, and why it matters
- ‚úÖ How Delta CDF works internally
- ‚úÖ How to process changes (insert, update, delete)
- ‚úÖ How to propagate DELETEs downstream
- ‚úÖ Batch and Streaming examples
- ‚úÖ Best practices and common gotchas

---

## üîç What is Change Data Capture (CDC)?

**CDC (Change Data Capture)** is a technique to track changes made to data ‚Äî such as:

- ‚ûï New rows added (INSERT)
- üìù Existing rows modified (UPDATE)
- ‚ùå Rows removed (DELETE)

Instead of reprocessing the **entire table**, CDC allows you to process **only the rows that changed** ‚Äî making data pipelines **faster, cheaper, and scalable**.

---

## üì¶ Why Use CDC?

| ‚úÖ Benefit                  | üìå Description                                               |
|----------------------------|--------------------------------------------------------------|
| Incremental processing     | Only changed rows are processed                             |
| Reduced compute cost       | Avoids full table scans                                     |
| Real-time sync             | Keeps downstream systems up to date quickly                 |
| Clean layering             | Efficient Bronze ‚Üí Silver ‚Üí Gold data movement              |
| Ideal for streaming        | Supports low-latency pipelines                              |

---

## üß† What is Delta Change Data Feed (CDF)?

Delta Lake‚Äôs **Change Data Feed (CDF)** is a **built-in CDC feature** that lets you **query only the changed rows** (insert/update/delete) from a Delta table.

It leverages Delta‚Äôs internal **transaction log (Delta log)** and exposes changes as a queryable DataFrame with metadata.

---

## üîÅ What Does CDF Capture?

| Operation | Description            | Captured in CDF? | Notes                         |
|-----------|------------------------|------------------|-------------------------------|
| INSERT    | New row added          | ‚úÖ Yes           | Captured as new data          |
| UPDATE    | Row values modified    | ‚úÖ Yes           | Before & after combined       |
| DELETE    | Row removed            | ‚úÖ Yes           | Columns null, key remains     |

---

## üõ†Ô∏è Step-by-Step: Enable and Use Delta CDF for CDC

---

### ‚úÖ Step 1: Enable Change Feed on the Source Table

```sql
ALTER TABLE customer_source
SET TBLPROPERTIES (
  delta.enableChangeDataFeed = true
);
```

> üî∏ You must enable CDF **before changes occur**. It only tracks future changes.

---

### ‚úÖ Step 2: Perform Some Changes (Insert / Update / Delete)

```sql
-- Insert
INSERT INTO customer_source VALUES (101, 'Amit', 'Pune');

-- Update
UPDATE customer_source SET city = 'Mumbai' WHERE customer_id = 101;

-- Delete
DELETE FROM customer_source WHERE customer_id = 101;
```

---

### ‚úÖ Step 3: Read the Changed Data using CDF

#### üìÑ Batch Mode

```python
cdf_df = spark.read.format("delta") \
    .option("readChangeData", "true") \
    .option("startingVersion", 5) \
    .table("customer_source")
```

You can also use:

```python
.option("startingTimestamp", "2025-07-01T00:00:00.000Z")
```

---

### ‚úÖ Step 4: Understand the CDC Output

| customer\_id | name | city   | \_change\_type | \_commit\_version |
| ------------ | ---- | ------ | -------------- | ----------------- |
| 101          | Amit | Pune   | insert         | 5                 |
| 101          | Amit | Mumbai | update         | 6                 |
| 101          | NULL | NULL   | delete         | 7                 |

> üî∏ Deleted rows will have `NULL` values except for key columns.

---

### ‚úÖ Step 5: Process CDC Feed using `MERGE INTO`

```python
cdf_df.createOrReplaceTempView("cdc_data")

spark.sql("""
MERGE INTO customer_target AS tgt
USING cdc_data AS src
ON tgt.customer_id = src.customer_id

WHEN MATCHED AND src._change_type = 'update' THEN
  UPDATE SET *

WHEN MATCHED AND src._change_type = 'delete' THEN
  DELETE

WHEN NOT MATCHED AND src._change_type = 'insert' THEN
  INSERT *
""")
```

‚úÖ This handles **insert, update, and delete** in a single query.

---

### ‚úÖ Step 6: (Optional) Use CDF with Streaming

```python
stream_df = spark.readStream.format("delta") \
    .option("readChangeData", "true") \
    .option("startingVersion", 20) \
    .table("customer_source")

stream_df.writeStream.foreachBatch(process_batch_fn).start()
```

Inside `process_batch_fn(df, batchId)`, apply the same `MERGE` logic.

---

## üîÅ CDF and Propagating DELETEs

---

### ‚úÖ How are DELETEs Represented in CDF?

When a row is deleted from the source:

* `_change_type = 'delete'`
* Non-key columns = `NULL`
* Key column (e.g., `customer_id`) is preserved

üìÑ Example:

| customer\_id | name | city | \_change\_type |
| ------------ | ---- | ---- | -------------- |
| 101          | NULL | NULL | delete         |

---

### ‚úÖ Why It‚Äôs Important?

In real-time systems, it‚Äôs critical to **remove deleted rows** from:

* Silver or Gold tables
* Reporting layers
* External sync systems (Redshift, Elasticsearch, etc.)

---

### ‚úÖ How to Apply DELETEs to Target

Handled inside `MERGE INTO` using:

```sql
WHEN MATCHED AND src._change_type = 'delete' THEN
  DELETE
```

---

### ‚úÖ If You Want to Handle DELETEs Separately:

```python
delete_df = cdf_df.filter("_change_type = 'delete'")
```

Apply direct delete like:

```python
delete_ids = delete_df.select("customer_id").rdd.flatMap(lambda x: x).collect()

spark.sql(f"""
DELETE FROM customer_target
WHERE customer_id IN ({','.join([str(x) for x in delete_ids])})
""")
```

---

## üìä Metadata Columns Added by Delta CDF

| Column              | Description                             |
| ------------------- | --------------------------------------- |
| `_change_type`      | `insert`, `update`, or `delete`         |
| `_commit_version`   | Delta log version where change occurred |
| `_commit_timestamp` | When the change was committed           |

---

## üìå Use Cases of CDC + Delta CDF

| Use Case                         | Purpose                                       |
| -------------------------------- | --------------------------------------------- |
| ‚è© Incremental ETL Pipelines      | Process only new and changed records          |
| üìä Real-Time Dashboards          | Keep BI data up-to-date                       |
| üîÅ External Sync                 | Push only deltas to Snowflake, Redshift, etc. |
| üßæ Audit / Compliance            | Track what changed, when, and by whom         |
| ü™Ñ Bronze ‚Üí Silver ‚Üí Gold Layers | Propagate changes efficiently across layers   |

---

## ‚ö†Ô∏è Best Practices

* ‚úÖ Always define `startingVersion` or `startingTimestamp`
* ‚úÖ Use strong match conditions in `MERGE`
* ‚úÖ Track `last_processed_version` in checkpoints or audit logs
* ‚úÖ Handle NULLs carefully in DELETE records
* ‚úÖ Use streaming with `foreachBatch()` for real-time sync

---

## ‚ùì FAQ

**Q: Does CDF track history before enabling it?**
‚û°Ô∏è No, only changes made **after enabling** CDF are tracked.

**Q: Can I use CDF with streaming?**
‚û°Ô∏è Yes, both batch and streaming are supported.

**Q: What happens if I forget `startingVersion`?**
‚û°Ô∏è You will get an error or unpredictable results. Always specify it.

---

## ‚úÖ Summary

| Topic                 | Description                                   |
| --------------------- | --------------------------------------------- |
| What is CDC?          | Capturing inserts, updates, deletes           |
| What is CDF?          | Delta Lake feature to query only changed data |
| Supports DELETEs?     | ‚úÖ Yes, with `_change_type = 'delete'`         |
| How to process?       | Use `MERGE`, `foreachBatch`, or SQL           |
| Works with streaming? | ‚úÖ Yes                                         |
| Output includes?      | Metadata columns with commit info             |
