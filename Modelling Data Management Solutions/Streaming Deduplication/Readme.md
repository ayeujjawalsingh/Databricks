# ğŸ” Streaming Deduplication

## ğŸ“˜ What is Streaming Deduplication?

When youâ€™re ingesting data using **streaming** (e.g., from Kafka, files, or APIs), **duplicate records** can come in due to:
- Retry logic in upstream systems
- Network failures
- Late arriving data

ğŸ‘‰ **Streaming Deduplication** means:
> Removing these duplicate records **before storing them**, so your Silver/Gold tables donâ€™t have repeated or incorrect data.

---

## â— Why Is Deduplication Important?

| Problem                        | Impact                          |
|--------------------------------|----------------------------------|
| âœ… Duplicate orders             | Double sales numbers             |
| âœ… Duplicate customer records   | Wrong customer counts            |
| âœ… Duplicate sensor readings    | Incorrect IoT analytics          |

ğŸ”§ You need **deduplication logic** to make your data **clean, accurate, and trustworthy**.

---

## ğŸ§  How to Identify Duplicates?

To detect duplicates, you need at least one of:

| Field             | Description |
|-------------------|-------------|
| **Unique ID**     | A key like `order_id`, `event_id`, etc. |
| **Event Timestamp**| When the event actually happened |
| **Ingestion Time** | When you received the data |

---

## ğŸ› ï¸ Deduplication in Streaming (Structured Streaming)

### âœ… Approach: Use `dropDuplicates()`

Spark supports **deduplication in streaming** using:

```python
dropDuplicates(["unique_column"])
```

Or based on a **combination of columns**:

```python
dropDuplicates(["order_id", "event_timestamp"])
```

---

## ğŸ§ª Example: Deduplicating Orders Stream

```python
stream_df = (spark.readStream
    .format("delta")
    .table("bronze.orders_raw"))

# Deduplicate using order_id
deduped_df = stream_df.dropDuplicates(["order_id"])

# Write to silver layer
deduped_df.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/chk/silver/orders/") \
    .table("silver.orders_cleaned")
```

---

## ğŸ§© Best Columns for Deduplication

| Column Type        | Use It When...                            |
| ------------------ | ----------------------------------------- |
| `event_id`         | You have a natural event key (e.g., UUID) |
| `order_id`         | In transactional systems like e-commerce  |
| `customer_id + ts` | For events per customer                   |
| `record_hash`      | Create a hash of important fields as ID   |

---

## âš ï¸ Important Notes on Streaming Deduplication

| Caution                                            | Explanation                                                  |
| -------------------------------------------------- | ------------------------------------------------------------ |
| Works only in `append` mode                        | Cannot deduplicate in `complete` or `update` modes           |
| Not guaranteed across all batches                  | Only deduplicates **within a single micro-batch** by default |
| Needs a **watermark** for time-based deduplication | Prevents memory overload when deduplicating by time          |

---

## â±ï¸ Deduplication with Watermark (for late events)

Sometimes events arrive **late**, and you want to deduplicate within a time window.

Use a **watermark** to define how late an event can be.

```python
from pyspark.sql.functions import expr

deduped_df = (stream_df
    .withWatermark("event_time", "10 minutes")
    .dropDuplicates(["order_id", "event_time"]))
```

ğŸ“ This means:

> Only keep the **first unique order\_id** per 10-minute window based on event time

---

## ğŸ’¡ Tip: Create a Hash Column for Deduplication

If you donâ€™t have a unique ID, you can **create a fingerprint** of the row:

```python
from pyspark.sql.functions import sha2, concat_ws

df = df.withColumn("record_hash", sha2(concat_ws("||", *df.columns), 256))
deduped_df = df.dropDuplicates(["record_hash"])
```

---

## âœ… Best Practices

| Best Practice                    | Why It Helps                 |
| -------------------------------- | ---------------------------- |
| Use unique keys if available     | Easy and fast                |
| Use combination of fields + time | Better for sensor / IoT data |
| Always use watermarks for time   | Controls memory usage        |
| Log dropped duplicates           | Helps in monitoring          |
| Use `record_hash` if no keys     | Flexible fallback option     |

---

## ğŸ“ Architecture Flow â€“ Streaming Deduplication

```
[ Raw Stream from Kafka or Autoloader ]
            |
            v
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 | dropDuplicates(["order_id"]) |
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            |
            v
[ Clean Data Stored in Silver Table ]
```

---

## ğŸ§¾ Summary â€“ Streaming Deduplication in a Nutshell

| âœ… What to Do                | ğŸ§  Why It Matters         |
| --------------------------- | ------------------------- |
| Use `dropDuplicates()`      | Remove duplicates easily  |
| Use watermark with time     | Handle late data properly |
| Use `record_hash` if needed | When no ID is available   |
| Log or store dropped rows   | For audit and debugging   |
| Store clean data in Silver  | Keep analytics accurate   |
