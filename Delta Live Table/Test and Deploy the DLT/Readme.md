# ðŸ“¦ Delta Live Tables - How to Test and Deploy

This guide explains how to **test** and **deploy** Delta Live Tables (DLT) pipelines in Databricks using simple steps.

---

## ðŸ§ª Step 1: How to Test Delta Live Tables

Before using a pipeline in production, always test it using **Development Mode**.

### ðŸ”§ 1. Create a Pipeline in Development Mode

Go to:
```

Workspace > Delta Live Tables > Create Pipeline

```

Fill the form with:

- **Name**: `my-pipeline-dev`
- **Notebook Path**: Where your DLT SQL/Python notebook is saved
- **Storage Location**: Temporary path like `/mnt/test-dlt-output`
- **Target Schema**: A testing database like `dev_dlt_db`
- **Pipeline Mode**: Select **Development**

âœ… Development mode helps you:
- Test changes without affecting real data
- Use sample/small data
- Quickly debug and re-run

---

### â–¶ï¸ 2. Run the Pipeline

Click **Start** on the DLT UI.

Youâ€™ll see a **graph view** showing the flow of your tables.

Watch for:
- âŒ Errors
- âœ… Successful steps
- ðŸ“‰ Dropped rows (if you added data quality checks)

---

### ðŸ” 3. Check and Validate Output

Use SQL to view the test output:

```sql
SELECT * FROM dev_dlt_db.clean_orders;
````

Make sure:

* Data is cleaned and transformed correctly
* No broken rows are included
* Your logic (filters, joins, etc.) is working as expected

---

## ðŸš€ Step 2: How to Deploy Delta Live Tables

Once your testing is done, you can safely deploy it using **Production Mode**.

### âš™ï¸ 1. Create a Pipeline in Production Mode

Create a new pipeline with:

* **Name**: `my-pipeline-prod`
* **Notebook Path**: Same DLT notebook as test
* **Storage Location**: `/mnt/prod-dlt-output`
* **Target Schema**: `prod_dlt_db`
* **Pipeline Mode**: Select **Production**

ðŸ”’ Production mode is:

* More stable and reliable
* Ideal for real, scheduled data processing

---

### ðŸ•’ 2. Schedule the Pipeline (Optional)

* You can **run manually**, or
* Set up a **schedule** (e.g., every 30 mins), or
* Trigger it using **Databricks Jobs** or **REST API**

---

### ðŸ“Š 3. Monitor the Pipeline

After deployment, Databricks shows a **monitoring UI** with:

| Tool             | What It Shows                    |
| ---------------- | -------------------------------- |
| ðŸ§¬ Lineage Graph | Flow between DLT tables          |
| ðŸ“ Run Logs      | Status, error logs, duration     |
| ðŸ›¡ï¸ Data Quality | Dropped rows and reasons         |
| ðŸ“ˆ Metrics       | Rows processed, time taken, etc. |

---

## ðŸ’¡ Best Practices

| âœ… Do This                   | ðŸ” Why It Helps                                        |
| --------------------------- | ------------------------------------------------------ |
| Use Dev Mode First          | To test logic and catch errors early                   |
| Separate Dev & Prod Schemas | Prevents accidental overwrite of real data             |
| Use `cloud_files()`         | Helps with streaming and auto-loading new data         |
| Add Constraints             | Cleans bad data before going to next table             |
| Use Git for Versioning      | Helps track changes in DLT logic                       |
| Monitor After Deployment    | Ensure data is flowing correctly and nothing is broken |

---

## ðŸ§ª Example: Add a Constraint for Data Quality

```sql
CREATE LIVE TABLE valid_orders
  CONSTRAINT valid_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW
AS
  SELECT * FROM live.clean_orders;
```

* This rule removes any row where `order_id` is empty
* Dropped rows are logged in the UI for tracking

---

## âœ… Summary

* ðŸ” Use **Development Mode** to test pipelines safely
* ðŸš€ Use **Production Mode** for scheduled, reliable runs
* ðŸ” Monitor the pipelines using the DLT UI
* ðŸ›¡ï¸ Add data rules to make your pipeline more robust
* ðŸ“‚ Separate environments for dev and prod are a must

> DLT makes your data pipeline development easier, safer, and more reliable â€” just test properly before deploying! âœ…
